# Awesome Unified Multimodal Models

## Survey 

### Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities [PDF](https://arxiv.org/pdf/2505.02567)

![](./figures/papers.png)
*Figure 1: Timeline of Publicly Available and Unavailable Unified Multimodal Models. The models are categorized by their release years, from 2023 to 2025. Models underlined in the diagram represent any-to-any multimodal models, capable of handling inputs or outputs beyond text and image, such as audio, video, and speech. The timeline highlights the rapid growth in this field.*
## Awesome Papers & Datasets
- [Text-and-Image Unified Models](#Text-and-Image-Unified-Models)
- [Any-to-Any Multimodal models](#Any-to-Any-Multimodal-models)
- [Benchmark for Evaluation](#Benchmark-for-Evaluation)
- [Dataset](#Dataset)


### Text-and-Image Unified Models
<a id="Text-and-Image-Unified-Models"></a>

| Title                                                                                                                                                                                                                                                           | Venue | Date       | Code                                                         | Demo                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---- | :--------- | :----------------------------------------------------------- | :------------------------------------------------------------------- |
| [Harmonizing Visual Representations for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2503.21979) ![GitHub Repo stars](https://img.shields.io/github/stars/wusize/Harmon?style=social)                                                 | ArXiv | 2025/04/22 | [Github](https://github.com/wusize/Harmon)                   | [Demo](https://huggingface.co/spaces/wusize/Harmon)                  |
| [Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads](https://arxiv.org/abs/2412.00127) ![GitHub Repo stars](https://img.shields.io/github/stars/zhijie-group/Orthus?style=social)                                            | ArXiv | 2025/04/16 | [Github](https://github.com/zhijie-group/Orthus)             | -                                                                    |
| [Liquid: Language Models are Scalable and Unified Multi-modal Generators](https://arxiv.org/abs/2412.04332) ![GitHub Repo stars](https://img.shields.io/github/stars/FoundationVision/Liquid?style=social)                                                      | ArXiv | 2025/04/10 | [Github](https://github.com/FoundationVision/Liquid)         | [Demo](https://huggingface.co/spaces/Junfeng5/Liquid_demo2025-04-10) |
| [Transfer between Modalities with MetaQueries](https://arxiv.org/abs/2504.06256)                                                                                                                                                                                | ArXiv | 2025/04/08 | -                                                            | -                                                                    |
| [VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via Iterative Instruction Tuning and Reinforcement Learning](https://arxiv.org/abs/2504.02949) ![GitHub Repo stars](https://img.shields.io/github/stars/VARGPT-family/VARGPT-v1.1?style=social) | ArXiv | 2025/04/03 | [Github](https://github.com/VARGPT-family/VARGPT-v1.1)       | -                                                                    |
| [ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement](https://arxiv.org/abs/2504.01934) ![GitHub Repo stars](https://img.shields.io/github/stars/illume-unified-mllm/ILLUME_plus?style=social)                            | ArXiv | 2025/04/03 | [Github](https://github.com/illume-unified-mllm/ILLUME_plus) | -                                                                    |
| [Dual Diffusion for Unified Image Generation and Understanding](https://arxiv.org/abs/2501.00289) ![GitHub Repo stars](https://img.shields.io/github/stars/zijieli-Jlee/Dual-Diffusion?style=social)                                                            | ArXiv | 2025/04/01 | [Github](https://github.com/zijieli-Jlee/Dual-Diffusion)     | -                                                                    |
| [UGen: Unified Autoregressive Multimodal Model with Progressive Vocabulary Learning](https://arxiv.org/abs/2503.21193)                                                                                                                                          | ArXiv | 2025/03/27 | -                                                            | -                                                                    |
| [JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2411.07975) ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/Janus?style=social)                       | CVPR  | 2025/03/24 | [Github](https://github.com/deepseek-ai/Janus)               | [Demo](https://huggingface.co/spaces/deepseek-ai/JanusFlow-1.3B)     |
| [Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://arxiv.org/abs/2405.09818) ![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/chameleon?style=social)                                                                     | ArXiv | 2025/03/21 | [Github](https://github.com/facebookresearch/chameleon)      | -                                                                    |
| [MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding](https://arxiv.org/abs/2411.17762)                                                                                                                                                            | ArXiv | 2025/03/19 | -                                                            | -                                                                    |
| [Unified Autoregressive Visual Generation and Understanding with Continuous Tokens](https://arxiv.org/abs/2503.13436)                                                                                                                                           | ArXiv | 2025/03/17 | -                                                            | -                                                                    |
| [OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models](https://arxiv.org/abs/2503.08686) ![GitHub Repo stars](https://img.shields.io/github/stars/hustvl/OmniMamba?style=social)                                     | ArXiv | 2025/03/11 | [Github](https://github.com/hustvl/OmniMamba)                | -                                                                    |
| [VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation](https://arxiv.org/abs/2409.04429) ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/vila-u?style=social)                                                | ICLR  | 2025/03/04 | [Github](https://github.com/mit-han-lab/vila-u)              | [Demo](https://vila-u.hanlab.ai/)                                    |
| [UniTok: A Unified Tokenizer for Visual Generation and Understanding](https://arxiv.org/abs/2502.20321) ![GitHub Repo stars](https://img.shields.io/github/stars/FoundationVision/UniTok?style=social)                                                          | ArXiv | 2025/02/27 | [Github](https://github.com/FoundationVision/UniTok)         | [Demo](https://huggingface.co/spaces/FoundationVision/UniTok)        |
| [LMFusion: Adapting Pretrained Language Models for Multimodal Generation](https://arxiv.org/abs/2412.15188)                                                                                                                                                     | ArXiv | 2025/02/05 | [Github](https://github.com/)                                | -                                                                    |
| [World Model on Million-Length Video And Language With Blockwise RingAttention](https://arxiv.org/abs/2402.08268) ![GitHub Repo stars](https://img.shields.io/github/stars/LargeWorldModel/LWM?style=social)                                                    | ICLR  | 2025/02/03 | [Github](https://github.com/LargeWorldModel/LWM)             | -                                                                    |
| [Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling](https://arxiv.org/abs/2501.17811) ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/Janus?style=social)                                             | ArXiv | 2025/01/29 | [Github](https://github.com/deepseek-ai/Janus)               | [Demo](https://huggingface.co/spaces/deepseek-ai/JanusFlow-1.3B)     |
| [VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model](https://arxiv.org/abs/2501.12327) ![GitHub Repo stars](https://img.shields.io/github/stars/VARGPT-family/VARGPT?style=social)                         | ArXiv | 2025/01/21 | [Github](https://github.com/VARGPT-family/VARGPT)            | -                                                                    |
| [MetaMorph: Multimodal Understanding and Generation via Instruction Tuning](https://arxiv.org/abs/2412.14164) ![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/metamorph?style=social)                                                 | ArXiv | 2024/12/18 | [Github](https://github.com/facebookresearch/metamorph/)     | -                                                                    |
| [ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance](https://arxiv.org/abs/2412.06673)                                                                                                                                                               | ArXiv | 2024/12/09 | -                                                            | -                                                                    |
| [SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding](https://arxiv.org/abs/2412.09604)                                                                                                                   | ArXiv | 2024/12/12 | -                                                            | -                                                                    |
| [TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation](https://arxiv.org/abs/2412.03069) ![GitHub Repo stars](https://img.shields.io/github/stars/ByteFlow-AI/TokenFlow?style=social)                                                 | CVPR  | 2024/12/04 | [Github](https://github.com/ByteFlow-AI/TokenFlow)           | -                                                                    |
| [Show-o: One Single Transformer to Unify Multimodal Understanding and Generation](https://arxiv.org/abs/2408.12528) ![GitHub Repo stars](https://img.shields.io/github/stars/showlab/Show-o?style=social)                                                       | ArXiv | 2024/10/21 | [Github](https://github.com/showlab/Show-o)                  | [Demo](https://huggingface.co/spaces/showlab/Show-o)                 |
| [PUMA: Empowering Unified MLLM with Multi-granular Visual Generation](https://arxiv.org/abs/2410.13861) ![GitHub Repo stars](https://img.shields.io/github/stars/rongyaofang/PUMA?style=social)                                                                 | ArXiv | 2024/10/21 | [Github](https://github.com/rongyaofang/PUMA)                | -                                                                    |
| [Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2410.13848) ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/Janus?style=social)                                              | ArXiv | 2024/10/17 | [Github](https://github.com/deepseek-ai/Janus)               | [Demo](https://huggingface.co/spaces/deepseek-ai/JanusFlow-1.3B)     |
| [MonoFormer: One Transformer for Both Diffusion and Autoregression](https://arxiv.org/abs/2409.16280) ![GitHub Repo stars](https://img.shields.io/github/stars/MonoFormer/MonoFormer?style=social)                                                              | ArXiv | 2024/10/15 | [Github](https://github.com/MonoFormer/MonoFormer)           | -                                                                    |
| [Emu3: Next-Token Prediction is All You Need](https://arxiv.org/abs/2409.18869) ![GitHub Repo stars](https://img.shields.io/github/stars/baaivision/Emu3?style=social)                                                                                          | ArXiv | 2024/09/27 | [Github](https://github.com/baaivision/Emu3)                 | [Demo](https://huggingface.co/spaces/BAAI/Emu3)                      |
| [MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling](https://arxiv.org/abs/2410.10798)                                                                                                                                                   | ArXiv | 2024/08/20 | -                                                            | -                                                                    |
| [Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model](https://arxiv.org/abs/2408.11039) ![GitHub Repo stars](https://img.shields.io/github/stars/lucidrains/transfusion-pytorch?style=social)                                     | ArXiv | 2024/08/20 | [Github](https://github.com/lucidrains/transfusion-pytorch)  | -                                                                    |
| [ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation](https://arxiv.org/abs/2407.06135) ![GitHub Repo stars](https://img.shields.io/github/stars/GAIR-NLP/anole?style=social)                                  | ArXiv | 2024/07/08 | [Github](https://github.com/GAIR-NLP/anole)                  | -                                                                    |
| [Generative Multimodal Models are In-Context Learners](https://arxiv.org/abs/2312.13286) ![GitHub Repo stars](https://img.shields.io/github/stars/baaivision/Emu?style=social)                                                                                  | CVPR  | 2024/05/08 | [Github](https://github.com/baaivision/Emu)                  | [Demo](http://218.91.113.230:9002/)                                  |
| [Emu: Generative Pretraining in Multimodality](https://arxiv.org/abs/2307.05222) ![GitHub Repo stars](https://img.shields.io/github/stars/baaivision/Emu?style=social)                                                                                          | ICLR  | 2024/05/08 | [Github](https://github.com/baaivision/Emu)                  | [Demo](http://218.91.113.230:9002/)                                  |
| [SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation](https://arxiv.org/pdf/2404.14396) ![GitHub Repo stars](https://img.shields.io/github/stars/AILab-CVC/SEED-X?style=social)                                               | ArXiv | 2024/05/02 | [Github](https://github.com/AILab-CVC/SEED-X)                | [Demo](https://arc.tencent.com/en/ai-demos/multimodal)               |
| [MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer](https://arxiv.org/abs/2401.10208) ![GitHub Repo stars](https://img.shields.io/github/stars/OpenGVLab/MM-Interleaved?style=social)                             | ArXiv | 2024/04/02 | [Github](https://github.com/OpenGVLab/MM-Interleaved)        | -                                                                    |
| [Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models](https://arxiv.org/abs/2403.18814) ![GitHub Repo stars](https://img.shields.io/github/stars/dvlab-research/MGM?style=social)                                                        | ArXiv | 2024/03/27 | [Github](https://github.com/dvlab-research/MGM)              | [Demo](http://103.170.5.190:7860/)                                   |
| [Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization](https://arxiv.org/abs/2309.04669) ![GitHub Repo stars](https://img.shields.io/github/stars/jy0205/LaVIT?style=social)                                                    | ICLR  | 2024/03/22 | [Github](https://github.com/jy0205/LaVIT)                    | -                                                                    |
| [DreamLLM: Synergistic Multimodal Comprehension and Creation](https://arxiv.org/abs/2309.11499) ![GitHub Repo stars](https://img.shields.io/github/stars/RunpeiDong/DreamLLM?style=social)                                                                      | ICLR  | 2024/03/15 | [Github](https://github.com/RunpeiDong/DreamLLM)             | -                                                                    |
| [VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation](https://arxiv.org/abs/2312.09251) ![GitHub Repo stars](https://img.shields.io/github/stars/AILab-CVC/VL-GPT?style=social)                                   | ArXiv | 2023/12/14 | [Github](https://github.com/AILab-CVC/VL-GPT)                | -                                                                    |
| [Making LLaMA SEE and Draw with SEED Tokenizer](https://arxiv.org/abs/2310.01218) ![GitHub Repo stars](https://img.shields.io/github/stars/AILab-CVC/SEED?style=social)                                                                                         | ICLR  | 2023/10/02 | [Github](https://github.com/AILab-CVC/SEED)                  | [Demo](https://huggingface.co/spaces/AILab-CVC/SEED-LLaMA)           |
| [Planting a SEED of Vision in Large Language Model](https://arxiv.org/abs/2307.08041) ![GitHub Repo stars](https://img.shields.io/github/stars/AILab-CVC/SEED?style=social)                                                                                     | ArXiv | 2023/08/12 | [Github](https://github.com/AILab-CVC/SEED)                  | [Demo](https://huggingface.co/spaces/AILab-CVC/SEED-LLaMA)           |

<!-- 
| Title | type | Venue | Date | Code | Demo |
|:------|:------|:------|:-----|:-----|:-----|
[World Model on Million-Length Video And Language With Blockwise RingAttention](https://arxiv.org/abs/2402.08268) ![GitHub Repo stars](https://img.shields.io/github/stars/LargeWorldModel/LWM?style=social) | AR. | ICLR 2025 | 2025-02-03 | [Github](https://github.com/LargeWorldModel/LWM) | - |
[Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://arxiv.org/abs/2405.09818) ![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/chameleon?style=social) | AR. | ArXiv | 2025-03-21 | [Github](https://github.com/facebookresearch/chameleon) | - |
[ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation](https://arxiv.org/abs/2407.06135) ![GitHub Repo stars](https://img.shields.io/github/stars/GAIR-NLP/anole?style=social) | AR. | ArXiv | 2024-07-08 | [Github](https://github.com/GAIR-NLP/anole) | - |
[Emu3: Next-Token Prediction is All You Need](https://arxiv.org/abs/2409.18869) ![GitHub Repo stars](https://img.shields.io/github/stars/baaivision/Emu3?style=social) | AR. | ArXiv | 2024-09-27 | [Github](https://github.com/baaivision/Emu3) | [Demo](https://huggingface.co/spaces/BAAI/Emu3) |
[VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation](https://arxiv.org/abs/2409.04429) ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/vila-u?style=social) | AR. | ICLR 2025 | 2025-03-04 | [Github](https://github.com/mit-han-lab/vila-u) | [Demo](https://vila-u.hanlab.ai/) |
[Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2410.13848) ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/Janus?style=social) | AR. | ArXiv | 2024-10-17 | [Github](https://github.com/deepseek-ai/Janus) | [Demo](https://huggingface.co/spaces/deepseek-ai/JanusFlow-1.3B) |
[MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding](https://arxiv.org/abs/2411.17762) | AR.| ArXiv | 2025-03-19 | - | - |
[Liquid: Language Models are Scalable and Unified Multi-modal Generators](https://arxiv.org/abs/2412.04332) ![GitHub Repo stars](https://img.shields.io/github/stars/FoundationVision/Liquid?style=social) | AR. | ArXiv | 2025-04-10 | [Github](https://github.com/FoundationVision/Liquid) | [Demo](https://huggingface.co/spaces/Junfeng5/Liquid_demo2025-04-10) |
[TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation](https://arxiv.org/abs/2412.03069) ![GitHub Repo stars](https://img.shields.io/github/stars/ByteFlow-AI/TokenFlow?style=social) | AR. | CVPR 2025 | 2024-12-04 | [Github](https://github.com/ByteFlow-AI/TokenFlow) | - |
[SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding](https://arxiv.org/abs/2412.09604) | AR. | ArXiv | 2024-12-12 | - | - |
[Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling](https://arxiv.org/abs/2501.17811) ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/Janus?style=social) | AR. | ArXiv | 2025-01-29 | [Github](https://github.com/deepseek-ai/Janus) | [Demo](https://huggingface.co/spaces/deepseek-ai/JanusFlow-1.3B) |
[UniTok: A Unified Tokenizer for Visual Generation and Understanding](https://arxiv.org/abs/2502.20321) ![GitHub Repo stars](https://img.shields.io/github/stars/FoundationVision/UniTok?style=social) | AR. | ArXiv | 2025/02/27 | [Github](https://github.com/FoundationVision/UniTok) | [Demo](https://huggingface.co/spaces/FoundationVision/UniTok) |
[UGen: Unified Autoregressive Multimodal Model with Progressive Vocabulary Learning](https://arxiv.org/abs/2503.21193) | AR. | ArXiv | 2025-03-27 | - | - |
[OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models](https://arxiv.org/abs/2503.08686) ![GitHub Repo stars](https://img.shields.io/github/stars/hustvl/OmniMamba?style=social) | AR. | ArXiv | 2025-03-11 | [Github](https://github.com/hustvl/OmniMamba) | - |
[Planting a SEED of Vision in Large Language Model](https://arxiv.org/abs/2307.08041) ![GitHub Repo stars](https://img.shields.io/github/stars/AILab-CVC/SEED?style=social) | Diff. | ArXiv | 2023-08-12 | [Github](https://github.com/AILab-CVC/SEED) | [Demo](https://huggingface.co/spaces/AILab-CVC/SEED-LLaMA) |
[Emu: Generative Pretraining in Multimodality](https://arxiv.org/abs/2307.05222) ![GitHub Repo stars](https://img.shields.io/github/stars/baaivision/Emu?style=social) | Diff. | ICLR 2024 | 2024-05-08 | [Github](https://github.com/baaivision/Emu) | [Demo](http://218.91.113.230:9002/) |
[DreamLLM: Synergistic Multimodal Comprehension and Creation](https://arxiv.org/abs/2309.11499) ![GitHub Repo stars](https://img.shields.io/github/stars/RunpeiDong/DreamLLM?style=social) | Diff. | ICLR 2024, Spotlight | 2024-03-15 | [Github](https://github.com/RunpeiDong/DreamLLM) | - |
[Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization](https://arxiv.org/abs/2309.04669) ![GitHub Repo stars](https://img.shields.io/github/stars/jy0205/LaVIT?style=social) | Diff. | ICLR 2024 | 2024-03-22 | [Github](https://github.com/jy0205/LaVIT) | - |
[Making LLaMA SEE and Draw with SEED Tokenizer](https://arxiv.org/abs/2310.01218) ![GitHub Repo stars](https://img.shields.io/github/stars/AILab-CVC/SEED?style=social) | Diff. | ICLR 2024 | 2023-10-02 | [Github](https://github.com/AILab-CVC/SEED) | [Demo](https://huggingface.co/spaces/AILab-CVC/SEED-LLaMA) |
[VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation](https://arxiv.org/abs/2312.09251) ![GitHub Repo stars](https://img.shields.io/github/stars/AILab-CVC/VL-GPT?style=social) | Diff. | ArXiv | 2023-12-14 | [Github](https://github.com/AILab-CVC/VL-GPT) | - |
[Generative Multimodal Models are In-Context Learners](https://arxiv.org/abs/2312.13286) ![GitHub Repo stars](https://img.shields.io/github/stars/baaivision/Emu?style=social) | Diff. | CVPR 2024 | 2024-05-08 | [Github](https://github.com/baaivision/Emu) | [Demo](http://218.91.113.230:9002/) |
[MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer](https://arxiv.org/abs/2401.10208) ![GitHub Repo stars](https://img.shields.io/github/stars/OpenGVLab/MM-Interleaved?style=social) | Diff. | ArXiv | 2024-04-02 | [Github](https://github.com/OpenGVLab/MM-Interleaved) | - |
[Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models](https://arxiv.org/abs/2403.18814) ![GitHub Repo stars](https://img.shields.io/github/stars/dvlab-research/MGM?style=social) | Diff. | ArXiv | 2024-03-27 | [Github](https://github.com/dvlab-research/MGM) | [Demo](http://103.170.5.190:7860/) |
[SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation](https://arxiv.org/pdf/2404.14396) ![GitHub Repo stars](https://img.shields.io/github/stars/AILab-CVC/SEED-X?style=social) | Diff. | ArXiv | 2024-05-02 | [Github](https://github.com/AILab-CVC/SEED-X) | [Demo](https://arc.tencent.com/en/ai-demos/multimodal) |
[PUMA: Empowering Unified MLLM with Multi-granular Visual Generation](https://arxiv.org/abs/2410.13861) ![GitHub Repo stars](https://img.shields.io/github/stars/rongyaofang/PUMA?style=social) | Diff. | ArXiv | 2024-10-21 | [Github](https://github.com/rongyaofang/PUMA) | - |
[Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads](https://arxiv.org/abs/2412.00127) ![GitHub Repo stars](https://img.shields.io/github/stars/zhijie-group/Orthus?style=social) | Diff. | ArXiv | 2025-04-16 | [Github](https://github.com/zhijie-group/Orthus) | - |
[MetaMorph: Multimodal Understanding and Generation via Instruction Tuning](https://arxiv.org/abs/2412.14164) ![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/metamorph?style=social) | Diff. | ArXiv | 2024-12-18 | [Github](https://github.com/facebookresearch/metamorph/) | - |
[ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance](https://arxiv.org/abs/2412.06673) | Diff. | ArXiv | 2024-12-09 | - | - |
[Unified Autoregressive Visual Generation and Understanding with Continuous Tokens](https://arxiv.org/abs/2503.13436) | Diff. | ArXiv | 2025-03-17 | - | - |
[ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement](https://arxiv.org/abs/2504.01934) ![GitHub Repo stars](https://img.shields.io/github/stars/illume-unified-mllm/ILLUME_plus?style=social) | AR+Diff | ArXiv | 2025-04-03 | [Github](https://github.com/illume-unified-mllm/ILLUME_plus) | - |
[Transfer between Modalities with MetaQueries](https://arxiv.org/abs/2504.06256)  | Diff. | ArXiv | 2025-04-08 | - | - |
[Harmonizing Visual Representations for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2503.21979) ![GitHub Repo stars](https://img.shields.io/github/stars/wusize/Harmon?style=social) | Diff. | ArXiv | 2025-04-22 | [Github](https://github.com/wusize/Harmon) | [Demo](https://huggingface.co/spaces/wusize/Harmon) |
[VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model](https://arxiv.org/abs/2501.12327) ![GitHub Repo stars](https://img.shields.io/github/stars/VARGPT-family/VARGPT?style=social) | AR. | ArXiv | 2025-01-21 | [Github](https://github.com/VARGPT-family/VARGPT) | - |
[VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via Iterative Instruction Tuning and Reinforcement Learning](https://arxiv.org/abs/2504.02949) ![GitHub Repo stars](https://img.shields.io/github/stars/VARGPT-family/VARGPT-v1.1?style=social) | AR. | ArXiv | 2025-04-03 | [Github](https://github.com/VARGPT-family/VARGPT-v1.1) | - |
[Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model](https://arxiv.org/abs/2408.11039) ![GitHub Repo stars](https://img.shields.io/github/stars/lucidrains/transfusion-pytorch?style=social) | AR+Diff | ArXiv | 2024-08-20 | [Github](https://github.com/lucidrains/transfusion-pytorch) | - |
[Show-o: One Single Transformer to Unify Multimodal Understanding and Generation](https://arxiv.org/abs/2408.12528) ![GitHub Repo stars](https://img.shields.io/github/stars/showlab/Show-o?style=social) | AR+Diff | ArXiv | 2024-10-21 | [Github](https://github.com/showlab/Show-o) | [Demo](https://huggingface.co/spaces/showlab/Show-o) |
[MonoFormer: One Transformer for Both Diffusion and Autoregression](https://arxiv.org/abs/2409.16280) ![GitHub Repo stars](https://img.shields.io/github/stars/MonoFormer/MonoFormer?style=social) | AR+Diff | ArXiv | 2024-10-15 | [Github](https://github.com/MonoFormer/MonoFormer) | - |
[MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling](https://arxiv.org/abs/2410.10798) | AR+Diff | ArXiv | 2024-08-20 | - | - |
[JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2411.07975) ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/Janus?style=social) | AR+Diff | CVPR 2025 | 2025-03-24 | [Github](https://github.com/deepseek-ai/Janus) | [Demo](https://huggingface.co/spaces/deepseek-ai/JanusFlow-1.3B) |
[LMFusion: Adapting Pretrained Language Models for Multimodal Generation](https://arxiv.org/abs/2412.15188) | AR+Diff | ArXiv | 2025-02-05 | [Github](https://github.com/) | - | -->

<!-- 
| Title | type | Venue | Date | Code | Demo |
|:------|:------|:------|:-----|:-----|:-----|
| [Dual Diffusion for Unified Image Generation and Understanding](https://arxiv.org/abs/2501.00289) ![GitHub Repo stars](https://img.shields.io/github/stars/zijieli-Jlee/Dual-Diffusion?style=social) | - | ArXiv | 2025-04-01 | [Github](https://github.com/zijieli-Jlee/Dual-Diffusion) | - |
[Planting a SEED of Vision in Large Language Model](https://arxiv.org/abs/2307.08041) ![GitHub Repo stars](https://img.shields.io/github/stars/AILab-CVC/SEED?style=social) | - | ArXiv | 2023-08-12 | [Github](https://github.com/AILab-CVC/SEED) | [Demo](https://huggingface.co/spaces/AILab-CVC/SEED-LLaMA) |
[Making LLaMA SEE and Draw with SEED Tokenizer](https://arxiv.org/abs/2310.01218) ![GitHub Repo stars](https://img.shields.io/github/stars/AILab-CVC/SEED?style=social) | - | ICLR 2024 | 2023-10-02 | [Github](https://github.com/AILab-CVC/SEED) | [Demo](https://c0c18bb50ca9b78b8f.gradio.live/) |
[World Model on Million-Length Video And Language With Blockwise RingAttention](https://arxiv.org/abs/2402.08268) ![GitHub Repo stars](https://img.shields.io/github/stars/LargeWorldModel/LWM?style=social) | - | ICLR 2025 | 2025-02-03 | [Github](https://github.com/LargeWorldModel/LWM) | - |
[Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://arxiv.org/abs/2405.09818) ![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/chameleon?style=social) | - | ArXiv | 2025-03-21 | [Github](https://github.com/facebookresearch/chameleon) | - |
[ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation](https://arxiv.org/abs/2407.06135) ![GitHub Repo stars](https://img.shields.io/github/stars/GAIR-NLP/anole?style=social) | - | ArXiv | 2024-07-08 | [Github](https://github.com/GAIR-NLP/anole) | - |
[SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation](https://arxiv.org/pdf/2404.14396) ![GitHub Repo stars](https://img.shields.io/github/stars/AILab-CVC/SEED-X?style=social) | - | ArXiv | 2024-05-02 | [Github](https://github.com/AILab-CVC/SEED-X) | [Demo](https://arc.tencent.com/en/ai-demos/multimodal) |
[Emu3: Next-Token Prediction is All You Need](https://arxiv.org/abs/2409.18869) ![GitHub Repo stars](https://img.shields.io/github/stars/baaivision/Emu3?style=social) | - | ArXiv | 2024-09-27 | [Github](https://github.com/baaivision/Emu3) | [Demo](https://huggingface.co/spaces/BAAI/Emu3) |
[VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation](https://arxiv.org/abs/2409.04429) ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/vila-u?style=social) | - | ICLR 2025 | 2025-03-04 | [Github](https://github.com/mit-han-lab/vila-u) | [Demo](https://vila-u.hanlab.ai/) |
[Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2410.13848) ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/Janus?style=social) | - | ArXiv | 2024-10-17 | [Github](https://github.com/deepseek-ai/Janus) | [Demo](https://huggingface.co/spaces/deepseek-ai/JanusFlow-1.3B) |
[MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding](https://arxiv.org/abs/2411.17762) | - | ArXiv | 2025-03-19 | - | - |
[Liquid: Language Models are Scalable and Unified Multi-modal Generators](https://arxiv.org/abs/2412.04332) ![GitHub Repo stars](https://img.shields.io/github/stars/FoundationVision/Liquid?style=social) | - | ArXiv | 2025-04-10 | [Github](https://github.com/FoundationVision/Liquid) | [Demo](https://huggingface.co/spaces/Junfeng5/Liquid_demo2025-04-10) |
[TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation](https://arxiv.org/abs/2412.03069) ![GitHub Repo stars](https://img.shields.io/github/stars/ByteFlow-AI/TokenFlow?style=social) | - | CVPR 2025 | 2024-12-04 | [Github](https://github.com/ByteFlow-AI/TokenFlow) | - |
[SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding](https://arxiv.org/abs/2412.09604) | - | ArXiv | 2024-12-12 | - | - |
[Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling](https://arxiv.org/abs/2501.17811) ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/Janus?style=social) | - | ArXiv | 2025-01-29 | [Github](https://github.com/deepseek-ai/Janus) | [Demo](https://huggingface.co/spaces/deepseek-ai/JanusFlow-1.3B) |
[UniTok: A Unified Tokenizer for Visual Generation and Understanding](https://arxiv.org/abs/2502.20321) ![GitHub Repo stars](https://img.shields.io/github/stars/FoundationVision/UniTok?style=social) | - | ArXiv | 2025/02/27 | [Github](https://github.com/FoundationVision/UniTok) | [Demo](https://huggingface.co/spaces/FoundationVision/UniTok) |
[UGen: Unified Autoregressive Multimodal Model with Progressive Vocabulary Learning](https://arxiv.org/abs/2503.21193) | - | ArXiv | 2025-03-27 | - | - |
[OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models](https://arxiv.org/abs/2503.08686) ![GitHub Repo stars](https://img.shields.io/github/stars/hustvl/OmniMamba?style=social) | - | ArXiv | 2025-03-11 | [Github](https://github.com/hustvl/OmniMamba) | - |
[Emu: Generative Pretraining in Multimodality](https://arxiv.org/abs/2307.05222) ![GitHub Repo stars](https://img.shields.io/github/stars/baaivision/Emu?style=social) | - | ICLR 2024 | 2024-05-08 | [Github](https://github.com/baaivision/Emu) | [Demo](http://218.91.113.230:9002/) |
[DreamLLM: Synergistic Multimodal Comprehension and Creation](https://arxiv.org/abs/2309.11499) ![GitHub Repo stars](https://img.shields.io/github/stars/RunpeiDong/DreamLLM?style=social) | - | ICLR 2024, Spotlight | 2024-03-15 | [Github](https://github.com/RunpeiDong/DreamLLM) | - |
[Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization](https://arxiv.org/abs/2309.04669) ![GitHub Repo stars](https://img.shields.io/github/stars/jy0205/LaVIT?style=social) | - | ICLR 2024 | 2024-03-22 | [Github](https://github.com/jy0205/LaVIT) | - |
[VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation](https://arxiv.org/abs/2312.09251) ![GitHub Repo stars](https://img.shields.io/github/stars/AILab-CVC/VL-GPT?style=social) | - | ArXiv | 2023-12-14 | [Github](https://github.com/AILab-CVC/VL-GPT) | - |
[Generative Multimodal Models are In-Context Learners](https://arxiv.org/abs/2312.13286) ![GitHub Repo stars](https://img.shields.io/github/stars/baaivision/Emu?style=social) | - | CVPR 2024 | 2024-5-8 | [Github](https://github.com/baaivision/Emu) | [Demo](http://218.91.113.230:9002/) |
[MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer](https://arxiv.org/abs/2401.10208) ![GitHub Repo stars](https://img.shields.io/github/stars/OpenGVLab/MM-Interleaved?style=social) | - | ArXiv | 2024-04-02 | [Github](https://github.com/OpenGVLab/MM-Interleaved) | - |
[Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models](https://arxiv.org/abs/2403.18814) ![GitHub Repo stars](https://img.shields.io/github/stars/dvlab-research/MGM?style=social) | - | ArXiv | 2024-03-27 | [Github](https://github.com/dvlab-research/MGM) | [Demo](http://103.170.5.190:7860/) |
[PUMA: Empowering Unified MLLM with Multi-granular Visual Generation](https://arxiv.org/abs/2410.13861) ![GitHub Repo stars](https://img.shields.io/github/stars/rongyaofang/PUMA?style=social) | - | ArXiv | 2024-10-21 | [Github](https://github.com/rongyaofang/PUMA) | - |
[Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads](https://arxiv.org/abs/2412.00127) ![GitHub Repo stars](https://img.shields.io/github/stars/zhijie-group/Orthus?style=social) | - | ArXiv | 2025-04-16 | [Github](https://github.com/zhijie-group/Orthus) | - |
[MetaMorph: Multimodal Understanding and Generation via Instruction Tuning](https://arxiv.org/abs/2412.14164) ![GitHub Repo stars](https://img.shields.io/github/stars/facebookresearch/metamorph/?style=social) | - | ArXiv | 2024-12-18 | [Github](https://github.com/facebookresearch/metamorph/) | - |
[ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance](https://arxiv.org/abs/2412.06673) | - | ArXiv | 2024-12-09 | - | - |
[Unified Autoregressive Visual Generation and Understanding with Continuous Tokens](https://arxiv.org/abs/2503.13436) | - | ArXiv | 2025-03-17 | - | - |
[ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement](https://arxiv.org/abs/2504.01934) ![GitHub Repo stars](https://img.shields.io/github/stars/illume-unified-mllm/ILLUME_plus?style=social) | - | ArXiv | 2025-04-03 | [Github](https://github.com/illume-unified-mllm/ILLUME_plus) | - |
[Transfer between Modalities with MetaQueries](https://arxiv.org/abs/2504.06256)  | - | ArXiv | 2025-04-08 | - | - | -->

<!-- #### Fused AR+Diff
| Title | type | Venue | Date | Code | Demo |
|:------|:------|:------|:-----|:-----|:-----| -->

<!-- [Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model](https://arxiv.org/abs/2408.11039) ![GitHub Repo stars](https://img.shields.io/github/stars/lucidrains/transfusion-pytorch?style=social) AR+Diff | ArXiv | 2024-08-20 | [Github](https://github.com/lucidrains/transfusion-pytorch) | - |
[Show-o: One Single Transformer to Unify Multimodal Understanding and Generation](https://arxiv.org/abs/2408.12528) ![GitHub Repo stars](https://img.shields.io/github/stars/showlab/Show-o?style=social) | AR+Diff | ArXiv | 2024-10-21 | [Github](https://github.com/showlab/Show-o) | [Demo](https://huggingface.co/spaces/showlab/Show-o) |
[MonoFormer: One Transformer for Both Diffusion and Autoregression](https://arxiv.org/abs/2409.16280) ![GitHub Repo stars](https://img.shields.io/github/stars/MonoFormer/MonoFormer?style=social) | AR+Diff | ArXiv | 2024-10-15 | [Github](https://github.com/MonoFormer/MonoFormer) | - |
[MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling](https://arxiv.org/abs/2410.10798) | AR+Diff | ArXiv | 2024-08-20 | - | - |
[JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2411.07975) ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/Janus?style=social) | AR+Diff | CVPR 2025 | 2025-03-24 | [Github](https://github.com/deepseek-ai/Janus) | [Demo](https://huggingface.co/spaces/deepseek-ai/JanusFlow-1.3B) |
[LMFusion: Adapting Pretrained Language Models for Multimodal Generation](https://arxiv.org/abs/2412.15188) | AR+Diff | ArXiv | 2025-02-05 | [Github](https://github.com/) | - | -->


#### Any-to-Any Multimodal models
<a id="Any-to-Any-Multimodal-models"></a>

| Title                                                                                                                                                                                                                        | Venue     | Date       | Code                                                | Demo |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------- | :--------- | :-------------------------------------------------- | :--- |
| [Spider: Any-to-many multimodal llm](https://arxiv.org/abs/2411.09439) ![GitHub Repo stars](https://img.shields.io/github/stars/Layjins/Spider?style=social)                                                                 | ArXiv     | 2025/04/07 | [Github](https://github.com/Layjins/Spider)         | -    |
| [Omniflow: Any-to-any generation with multi-modal rectified flows.](https://arxiv.org/abs/2412.01169) ![GitHub Repo stars](https://img.shields.io/github/stars/jacklishufan/OmniFlows?style=social)                          | CVPR 2025 | 2025/03/21 | [Github](https://github.com/jacklishufan/OmniFlows) | -    |
| [Mio: A foundation model on multimodal tokens](https://arxiv.org/abs/2409.17692) ![GitHub Repo stars](https://img.shields.io/github/stars/MIO-Team/MIO?style=social)                                                         | ArXiv     | 2025/01/13 | [Github](https://github.com/MIO-Team/MIO)           |      |
| [NExT-GPT: Any-to-any multi-modal llm](https://arxiv.org/abs/2309.05519) ![GitHub Repo stars](https://img.shields.io/github/stars/NExT-GPT/NExT-GPT?style=social)                                                            | ICML 2024 | 2024/06/25 | [Github](https://github.com/NExT-GPT/NExT-GPT)      | -    |
| [Video-lavit: Unified video-language pre-training with decoupled visual-motional tokenization](https://arxiv.org/abs/2402.03161) ![GitHub Repo stars](https://img.shields.io/github/stars/jy0205/LaVIT?style=social)         | ICML 2024 | 2024/06/03 | [Github](https://github.com/jy0205/LaVIT)           | -    |
| [X-vila: Cross-modality alignment for large language model](https://arxiv.org/abs/2405.19335)                                                                                                                                | ArXiv     | 2024/05/29 | -                                                   | -    |
| [Anygpt: Unified multimodal llm with discrete sequence modeling](https://arxiv.org/abs/2402.12226) ![GitHub Repo stars](https://img.shields.io/github/stars/OpenMOSS/AnyGPT?style=social)                                    | ArXiv     | 2024/03/07 | [Github](https://github.com/OpenMOSS/AnyGPT)        | -    |
| [Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action](https://arxiv.org/abs/2312.17172) ![GitHub Repo stars](https://img.shields.io/github/stars/allenai/unified-io-2?style=social) | CVPR 2024 | 2023/12/28 | [Github](https://github.com/allenai/unified-io-2)   | -    |

### Benchmark for Evaluation
<a id="Benchmark-for-Evaluation"></a>

|       Name       | Paper                                                                                                                                                                                                                                                                 |  Venue  |    Date    |                                    Code                                    |
| :--------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-----: | :--------: | :-------------------------------------------------------------------------: |
|   GEdit-Bench   | ![Star](https://img.shields.io/github/stars/stepfun-ai/Step1X-Edit.svg?style=social&label=Star) <br> [**Step1X-Edit: A Practical Framework for General Image Editing**](https://arxiv.org/html/2504.17761v2) <br>                                                     |  arxiv  | 2025-04-28 |             [Github](https://github.com/stepfun-ai/Step1X-Edit)             |
|     OpenING     | ![Star](https://img.shields.io/github/stars/LanceZPF/OpenING.svg?style=social&label=Star) <br> [**OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation**](https://arxiv.org/abs/2411.18499) <br>                               |  CVPR  | 2024-11-27 |                [Github](https://github.com/LanceZPF/OpenING)                |
|       ISG       | ![Star](https://img.shields.io/github/stars/Dongping-Chen/ISG.svg?style=social&label=Star) <br> [**Interleaved Scene Graphs for Interleaved Text-and-Image Generation Assessment**](https://arxiv.org/abs/2411.17188) <br>                                            |  ICLR  | 2024-11-26 |               [Github](https://github.com/Dongping-Chen/ISG)               |
|       MMIE       | ![Star](https://img.shields.io/github/stars/Lillianwei-h/MMIE.svg?style=social&label=Star) <br> [**MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models**](https://arxiv.org/abs/2410.10139) <br>                            |  ICLR  | 2024-10-14 |               [Github](https://github.com/Lillianwei-h/MMIE)               |
|    MM-Vet v2    | ![Star](https://img.shields.io/github/stars/yuweihao/MM-Vet.svg?style=social&label=Star) <br> [**MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models for Integrated Capabilities**](https://arxiv.org/abs/2408.00765) <br>                         |  arxiv  | 2024-08-01 |          [Github](https://github.com/yuweihao/MM-Vet/tree/main/v2)          |
| InterleavedBench | [**Holistic Evaluation for Interleaved Text-and-Image Generation**](https://arxiv.org/abs/2406.14643) <br>                                                                                                                                                            |  EMNLP  | 2024-06-20 |        [HuggingFace](https://huggingface.co/mqliu/InterleavedBench)        |
|     OwlEval     | ![Star](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?style=social&label=Star) <br> [**mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality**](https://arxiv.org/abs/2304.14178) <br>                                               |  arixv  | 2024-04-27 |                [Github](https://github.com/X-PLUG/mPLUG-Owl)                |
|    DPG-Bench    | ![Star](https://img.shields.io/github/stars/TencentQQGYLab/ELLA.svg?style=social&label=Star) <br> [**ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment**](https://arxiv.org/abs/2403.05135) <br>                                                  |  arxiv  | 2024-03-08 |    [Github](https://github.com/TencentQQGYLab/ELLA/tree/main/dpg_bench)    |
|       oVQA       | ![Star](https://img.shields.io/github/stars/lmb-freiburg/ovqa.svg?style=social&label=Star) <br> [**Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy**](https://arxiv.org/abs/2402.07270) <br> |  ICLR  | 2024-02-11 |               [Github](https://github.com/lmb-freiburg/ovqa)               |
|   SEED-Bench-2   | ![Star](https://img.shields.io/github/stars/AILab-CVC/SEED-Bench.svg?style=social&label=Star) <br> [**SEED-Bench-2: Benchmarking Multimodal Large Language Models**](https://arxiv.org/abs/2311.17092) <br>                                                           |  arixv  | 2023-11-28 |  [Github](https://github.com/AILab-CVC/SEED-Bench/tree/main/SEED-Bench-2)  |
|       MMMU       | ![Star](https://img.shields.io/github/stars/MMMU-Benchmark/MMMU.svg?style=social&label=Star) <br> [**MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI**](https://arxiv.org/abs/2311.16502) <br>                       |  CVPR  | 2023-11-27 |              [Github](https://github.com/MMMU-Benchmark/MMMU)              |
|     Emu Edit     | [**Emu Edit: Precise Image Editing via Recognition and Generation Tasks**](https://arxiv.org/abs/2311.10089) <br>                                                                                                                                                     |  CVPR  | 2023-11-16 |  [HuggingFace](https://huggingface.co/datasets/facebook/emu_edit_test_set)  |
|     GenEval     | ![Star](https://img.shields.io/github/stars/djghosh13/geneval.svg?style=social&label=Star) <br> [**GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment**](https://arxiv.org/abs/2310.11513) <br>                                              | NeurIPS | 2023-10-17 |               [Github](https://github.com/djghosh13/geneval)               |
|      MM-Vet      | ![Star](https://img.shields.io/github/stars/yuweihao/MM-Vet.svg?style=social&label=Star) <br> [**MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities**](https://arxiv.org/abs/2308.02490) <br>                                                     |  ICML  | 2023-08-04 |                [Github](https://github.com/yuweihao/MM-Vet)                |
|    SEED-Bench    | ![Star](https://img.shields.io/github/stars/AILab-CVC/SEED-Bench.svg?style=social&label=Star) <br> [**SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension**](https://arxiv.org/abs/2307.16125) <br>                                                |  CVPR  | 2023-07-30 |              [Github](https://github.com/AILab-CVC/SEED-Bench)              |
|     MMBench     | ![Star](https://img.shields.io/github/stars/open-compass/MMBench.svg?style=social&label=Star) <br> [**MMBench: Is Your Multi-modal Model an All-around Player?**](https://arxiv.org/abs/2307.06281) <br>                                                              |  ECCV  | 2023-07-12 |              [Github](https://github.com/open-compass/MMBench)              |
|    MagicBrush    | ![Star](https://img.shields.io/github/stars/OSU-NLP-Group/MagicBrush.svg?style=social&label=Star) <br> [**MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing**](https://arxiv.org/abs/2306.10012) <br>                                     | NeurIPS | 2023-06-16 |            [Github](https://github.com/OSU-NLP-Group/MagicBrush)            |
|       LAMM       | ![Star](https://img.shields.io/github/stars/OpenLAMM/LAMM.svg?style=social&label=Star) <br> [**LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark**](https://arxiv.org/abs/2306.06687) <br>                                     | NeurIPS | 2023-06-11 |                 [Github](https://github.com/OpenLAMM/LAMM)                 |
|     HaluEval     | ![Star](https://img.shields.io/github/stars/RUCAIBox/HaluEval?style=social&label=Star) <br> [**HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models**](https://arxiv.org/abs/2305.11747) <br>                                         |  EMNLP  | 2023-05-19 |               [Github](https://github.com/RUCAIBox/HaluEval)               |
|       GQA       | ![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star) <br> [**GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering**](https://arxiv.org/abs/1902.09506) <br>                                   |  CVPR  | 2019-02-25 | [Github](https://github.com/salesforce/LAVIS/blob/main/dataset_card/gqa.md) |
|       VQA       | [**VQA: Visual Question Answering**](https://arxiv.org/abs/1505.00468) <br>                                                                                                                                                                                           |  ICCV  | 2015-05-03 |               [ProjectPage](https://visualqa.org/index.html)               |

### Dataset
<a id="Dataset"></a>

| Type                      | Dataset                                                                                | Samples | Paper                                                                                                                                                                                                                                                                                                                                                                                                      | Venue   | Date       |
| :------------------------ | :------------------------------------------------------------------------------------- | :------ | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------ | :--------- |
| Multimodal Understanding  | [ShareGPT4V](https://huggingface.co/datasets/Lin-Chen/ShareGPT4V)                      | 100K    | [Sharegpt4v: Improving large multi-modal models with better captions](https://arxiv.org/pdf/2311.12793)                                                                                                                                                                                                                                                                                                    | ECCV    | 2023/11/21 |
| Multimodal Understanding  | [CapsFusion-120M](http://huggingface.co/datasets/BAAI/CapsFusion-120M)                 | 120M    | [Capsfusion: Rethinking image-text data at scale](https://arxiv.org/pdf/2310.20550)                                                                                                                                                                                                                                                                                                                        | CVPR    | 2023/10/31 |
| Multimodal Understanding  | [GRIT](https://huggingface.co/datasets/zzliang/GRIT)                                   | 20M     | [Kosmos-2: Grounding multimodal large language models to the world](https://arxiv.org/pdf/2306.14824)                                                                                                                                                                                                                                                                                                      | ICLR    | 2023/06/26 |
| Multimodal Understanding  | [DataComp](https://huggingface.co/datasets/mlfoundations/datacomp_1b)                  | 1.4B    | [DATACOMP: In search of the next generation of multimodal datasets](https://arxiv.org/pdf/2304.14108)                                                                                                                                                                                                                                                                                                      | NeurIPS | 2023/04/27 |
| Multimodal Understanding  | [SAM](https://segment-anything.com/dataset/index.html)                                 | 11M     | [Segment Anything](https://scontent-dfw5-2.xx.fbcdn.net/v/t39.2365-6/10000000_900554171201033_1602411987825904100_n.pdf?_nc_cat=100&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=B3oBIrInbQUQ7kNvwEaPRAg&_nc_oc=Admn--QE4uSxaSrSevMzE9NUEkdPzlxF28dIu1Pi3-T9Wv87G_eomLxfVv1_LurC1lk&_nc_zt=14&_nc_ht=scontent-dfw5-2.xx&_nc_gid=RT_BFXUYrx0OfvqBSy2btQ&oh=00_AfG2YboYUGvYXugQ45dgIz3h8g0B8YDqxpf0ra9lVAa_EQ&oe=6816D3A7) | ICCV    | 2023/04/05 |
| Multimodal Understanding  | [Laion-COCO](https://huggingface.co/datasets/laion/laion-coco)                         | 600M    | [Laion coco: 600m synthetic captions from laion2b-en](https://laion.ai/blog/laion-coco/)                                                                                                                                                                                                                                                                                                                   | -       | 2022/09/15 |
| Multimodal Understanding  | [COYO](https://huggingface.co/datasets/kakaobrain/coyo-700m)                           | 747M    | [Coyo-700m: Image-text pair dataset](https://github.com/kakaobrain/coyo-dataset)                                                                                                                                                                                                                                                                                                                           | -       | 2022/08/31 |
| Multimodal Understanding  | [Laion](https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/) | 5.9B    | [Laion-5b: An open large-scale dataset for training next generation image-text models](https://arxiv.org/pdf/2210.08402)                                                                                                                                                                                                                                                                                   | NeurIPS | 2022/03/31 |
| Multimodal Understanding  | [Wukong](https://wukong-dataset.github.io/wukong-dataset/)                             | 100M    | [Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark](https://arxiv.org/pdf/2202.06767)                                                                                                                                                                                                                                                                                           | NeurIPS | 2022/02/14 |
| Multimodal Understanding  | [RedCaps](https://huggingface.co/datasets/kdexd/red_caps)                              | 12M     | [Redcaps: Web-curated image-text data created by the people, for the people](https://arxiv.org/pdf/2111.11431)                                                                                                                                                                                                                                                                                             | NeurIPS | 2021/11/22 |
| Text-to-Image             | [PD12M](https://huggingface.co/datasets/Spawning/PD12M)                                | 12M     | [Public domain 12m: A highly aesthetic image-text dataset with novel governance mechanisms](https://arxiv.org/pdf/2410.23144)                                                                                                                                                                                                                                                                              | arXiv   | 2024/10/30 |
| Text-to-Image             | [DenseFusion](https://huggingface.co/datasets/BAAI/DenseFusion-1M)                     | 1M      | [Densefusion-1m: Merging vision experts for comprehensive multimodal perception](https://arxiv.org/pdf/2407.08303)                                                                                                                                                                                                                                                                                         | NeurIPS | 2024/07/11 |
| Text-to-Image             | [Megalith](https://huggingface.co/datasets/madebyollin/megalith-10m)                   | 10M     | -                                                                                                                                                                                                                                                                                                                                                                                                          | -       | 2024/07/01 |
| Text-to-Image             | [PixelProse](https://huggingface.co/datasets/tomg-group-umd/pixelprose)                | 16M     | [From pixels to prose: A large dataset of dense image captions](https://arxiv.org/pdf/2406.10328)                                                                                                                                                                                                                                                                                                          | arXiv   | 2024/06/14 |
| Text-to-Image             | [CosmicMan-HQ 1.0](https://huggingface.co/datasets/cosmicman/CosmicManHQ-1.0)          | 6M      | [Cosmicman: A text-to-image foundation model for humans](https://arxiv.org/pdf/2404.01294)                                                                                                                                                                                                                                                                                                                 | CVPR    | 2024/04/01 |
| Text-to-Image             | [AnyWord-3M](https://huggingface.co/datasets/stzhao/AnyWord-3M)                        | 3M      | [Anytext: Multilingual visual text generation and editing](https://arxiv.org/pdf/2311.03054)                                                                                                                                                                                                                                                                                                               | ICLR    | 2023/11/06 |
| Text-to-Image             | [JourneyDB](https://huggingface.co/datasets/JourneyDB/JourneyDB)                       | 4M      | [JourneyDB: A Benchmark for Generative Image Understanding](https://arxiv.org/pdf/2307.00716)                                                                                                                                                                                                                                                                                                              | NeurIPS | 2023/07/03 |
| Text-to-Image             | [Mario-10M](https://huggingface.co/datasets/JingyeChen22/TextDiffuser-MARIO-10M)       | 10M     | [Textdiffuser: Diffusion models as text painters](https://arxiv.org/pdf/2305.10855)                                                                                                                                                                                                                                                                                                                        | NeurIPS | 2023/05/18 |
| Text-to-Image             | [SAM](https://segment-anything.com/dataset/index.html)                                 | 11M     | [Segment Anything](https://scontent-dfw5-2.xx.fbcdn.net/v/t39.2365-6/10000000_900554171201033_1602411987825904100_n.pdf?_nc_cat=100&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=B3oBIrInbQUQ7kNvwEaPRAg&_nc_oc=Admn--QE4uSxaSrSevMzE9NUEkdPzlxF28dIu1Pi3-T9Wv87G_eomLxfVv1_LurC1lk&_nc_zt=14&_nc_ht=scontent-dfw5-2.xx&_nc_gid=RT_BFXUYrx0OfvqBSy2btQ&oh=00_AfG2YboYUGvYXugQ45dgIz3h8g0B8YDqxpf0ra9lVAa_EQ&oe=6816D3A7) | ICCV    | 2023/04/05 |
| Text-to-Image             | [LAION-Aesthetics](https://laion.ai/blog/laion-aesthetics/)                            | 120M    | [Laion-5b: An open large-scale dataset for training next generation image-text models](https://arxiv.org/pdf/2210.08402)                                                                                                                                                                                                                                                                                   | NeurIPS | 2022/08/16 |
| Text-to-Image             | [CC-12M](https://github.com/google-research-datasets/conceptual-12m)                   | 12M     | [Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts](https://arxiv.org/pdf/2102.08981)                                                                                                                                                                                                                                                                       | CVPR    | 2021/02/17 |
| Image Editing             | [AnyEdit](https://huggingface.co/datasets/Bin1117/AnyEdit)                             | 2.5M    | [Anyedit: Mastering unified high-quality image editing for any idea](https://arxiv.org/pdf/2411.15738)                                                                                                                                                                                                                                                                                                     | CVPR    | 2024/11/24 |
| Image Editing             | [OmniEdit](https://huggingface.co/datasets/TIGER-Lab/OmniEdit-Filtered-1.2M)           | 1.2M    | [Omniedit: Building image editing generalist models through specialist supervision](https://arxiv.org/pdf/2411.07199)                                                                                                                                                                                                                                                                                      | ICLR    | 2024/11/11 |
| Image Editing             | [UltraEdit](https://huggingface.co/datasets/BleachNick/UltraEdit)                      | 4M      | [Ultraedit: Instruction-based fine-grained image editing at scale](https://arxiv.org/pdf/2407.05282)                                                                                                                                                                                                                                                                                                       | NeurIPS | 2024/07/07 |
| Image Editing             | [SEED-Data-Edit](https://huggingface.co/datasets/AILab-CVC/SEED-Data-Edit)             | 3.7M    | [Seed-data-edit technical report: A hybrid dataset for instructional image editing](https://arxiv.org/pdf/2405.04007)                                                                                                                                                                                                                                                                                      | arXiv   | 2024/05/07 |
| Image Editing             | [HQ-Edit](https://huggingface.co/datasets/UCSC-VLAA/HQ-Edit)                           | 197K    | [Hq-edit: A high-quality dataset for instruction-based image editing](https://arxiv.org/pdf/2404.09990)                                                                                                                                                                                                                                                                                                    | arXiv   | 2024/04/15 |
| Image Editing             | [Magicbrush](https://huggingface.co/datasets/osunlp/MagicBrush)                        | 10K     | [Magicbrush: A manually annotated dataset for instruction-guided image editing](https://arxiv.org/pdf/2306.10012)                                                                                                                                                                                                                                                                                          | NeurIPS | 2023/06/16 |
| Image Editing             | [InstructP2P](https://github.com/timothybrooks/instruct-pix2pix)                       | 313K    | [nstructpix2pix: Learning to follow image editing instructions](https://arxiv.org/pdf/2211.09800)                                                                                                                                                                                                                                                                                                          | CVPR    | 2022/11/17 |
| Interleaved Image-Text    | [CoMM](https://huggingface.co/datasets/HuggingFaceM4/OBELICS)                          | 227K    | [Comm: A coherent interleaved image-text dataset for multimodal understanding and generation](https://arxiv.org/pdf/2406.10462)                                                                                                                                                                                                                                                                            | CVPR    | 2024/06/15 |
| Interleaved Image-Text    | [OBELICS](https://huggingface.co/datasets/HuggingFaceM4/OBELICS)                       | 141M    | [Obelics: An open web-scale filtered dataset of interleaved image-text documents](https://arxiv.org/pdf/2306.16527)                                                                                                                                                                                                                                                                                        | NeurIPS | 2023/06/21 |
| Interleaved Image-Text    | [Multimodal C4](https://github.com/allenai/mmc4)                                       | 101.2M  | [Multimodal c4: An open, billion-scale corpus of images interleaved with text](https://arxiv.org/pdf/2304.06939)                                                                                                                                                                                                                                                                                           | NeurIPS | 2023/04/14 |
| Other Text+Image-to-Image | [SynCD](https://huggingface.co/datasets/nupurkmr9/syncd)                               | 95K     | [Generating multi-image synthetic data for text-to-image customization](https://arxiv.org/pdf/2502.01720)                                                                                                                                                                                                                                                                                                  | arXiv   | 2025/02/03 |
| Other Text+Image-to-Image | [Subjects200K](https://huggingface.co/datasets/Yuanshi/Subjects200K)                   | 200K    | [Ominicontrol: Minimal and universal control for diffusion transformer](https://arxiv.org/pdf/2411.15098)                                                                                                                                                                                                                                                                                                  | arXiv   | 2024/11/22 |
| Other Text+Image-to-Image | [MultiGen-20M](https://huggingface.co/datasets/limingcv/MultiGen-20M_train)            | 20M     | [Unicontrol: A unified diffusion model for controllable visual generation in the wild](https://arxiv.org/pdf/2305.11147)                                                                                                                                                                                                                                                                                   | NeurIPS | 2023/05/18 |
| Other Text+Image-to-Image | [LAION-Face](https://huggingface.co/datasets/FacePerceiver/laion-face)                 | 50M     | [eneral facial representation learning in a visual-linguistic manner](https://arxiv.org/pdf/2112.03109)                                                                                                                                                                                                                                                                                                    | CVPR    | 2021/12/06 |
